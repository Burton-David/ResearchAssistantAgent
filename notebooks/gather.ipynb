{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Cell 1: Initial Setup and Essential Imports\n",
    "import os\n",
    "import logging\n",
    "import asyncio\n",
    "import json\n",
    "import nest_asyncio\n",
    "from datetime import datetime as dt\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, HttpUrl\n",
    "\n",
    "# Load environment variables (API keys, etc.)\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configure logging\n",
    "LOG_FILE = 'research_collector.log'\n",
    "LOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=LOG_FORMAT,\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_FILE),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.propagate = False  # Prevent duplicate logging\n",
    "\n",
    "# Load environment variables\n",
    "# Load environment variables\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path(\"/Users/davidburton/src/research_paper_analysis\")\n",
    "ENV_PATH = PROJECT_ROOT / \"config\" / \".env\"\n",
    "load_dotenv(ENV_PATH)\n",
    "\n",
    "# Verify environment variables\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    logger.info(\"OpenAI API key loaded successfully\")\n",
    "else:\n",
    "    logger.warning(\"OpenAI API key not found in environment variables\")\n",
    "    \n",
    "# Apply nest_asyncio only in Jupyter environments\n",
    "try:\n",
    "    get_ipython  # Check if running in Jupyter\n",
    "    nest_asyncio.apply()\n",
    "    logger.info(\"Applied nest_asyncio for async support in Jupyter.\")\n",
    "except NameError:\n",
    "    pass  # Skip in standard Python scripts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Cell 2: Core Data Models\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field, HttpUrl, field_serializer\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "class Author(BaseModel):\n",
    "    \"\"\"Author information model\"\"\"\n",
    "    name: str\n",
    "    affiliations: List[str] = Field(default_factory=list)\n",
    "\n",
    "class ResearchPaper(BaseModel):\n",
    "    \"\"\"Enhanced research paper model with vector storage support\"\"\"\n",
    "    title: str\n",
    "    authors: List[Author]\n",
    "    year: Optional[int] = None\n",
    "    abstract: Optional[str] = None\n",
    "    full_text: Optional[str] = None\n",
    "    doi: Optional[str] = Field(\n",
    "        None,\n",
    "        pattern=r\"^10\\.\\d{4,9}/.+\",\n",
    "        description=\"DOI must be a valid identifier starting with '10.'\"\n",
    "    )\n",
    "    url: Optional[HttpUrl] = None\n",
    "    venue: Optional[str] = None\n",
    "    citation_count: int = Field(default=0, ge=0)\n",
    "    source: str\n",
    "    embedding: Optional[List[float]] = None\n",
    "    collection_timestamp: datetime = Field(default_factory=lambda: datetime.utcnow().replace(tzinfo=None))\n",
    "    metadata: Dict = Field(default_factory=dict)\n",
    "\n",
    "    def get_content_for_embedding(self) -> str:\n",
    "        \"\"\"Generate content for embedding creation\"\"\"\n",
    "        content_parts = [\n",
    "            self.title,\n",
    "            self.abstract or \"\",\n",
    "            self.full_text or \"\",\n",
    "            self.venue or \"\",\n",
    "            \" \".join(a.name for a in self.authors)\n",
    "        ]\n",
    "        return \"\\n\".join(filter(None, content_parts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Cell 3: Enhanced Research Collection Base System\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import hashlib\n",
    "from typing import Optional, List, Dict\n",
    "import json\n",
    "\n",
    "class ResearchCollector:\n",
    "    \"\"\"Base system for collecting and storing academic papers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize logging\n",
    "        self.logger = logging.getLogger(__name__ + \".ResearchCollector\")\n",
    "        \n",
    "        # Initialize cache directory\n",
    "        self.cache_dir = Path(\"research_cache\")\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Initialize rate limiting\n",
    "        self.request_delay = 1.0\n",
    "        self.last_request_time = time.monotonic()\n",
    "        \n",
    "        # Initialize storage\n",
    "        self.papers_collected: List[Dict] = []\n",
    "        self.cache_ttl_days = 7\n",
    "\n",
    "    def _get_cache_key(self, query: str, source: str) -> str:\n",
    "        \"\"\"Generate a unique cache key for a query.\"\"\"\n",
    "        sanitized_query = query.strip().lower()\n",
    "        return f\"{source}_{hashlib.md5(sanitized_query.encode()).hexdigest()}\"\n",
    "\n",
    "    def _check_cache(self, query: str, source: str) -> list:\n",
    "        \"\"\"Check if results exist in cache and are still valid.\"\"\"\n",
    "        cache_file = self.cache_dir / f\"{self._get_cache_key(query, source)}.json\"\n",
    "\n",
    "        if not cache_file.exists():\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            with cache_file.open('r', encoding=\"utf-8\") as f:\n",
    "                cache_data = json.load(f)\n",
    "\n",
    "            timestamp = cache_data.get('timestamp')\n",
    "            if not timestamp:\n",
    "                return []\n",
    "\n",
    "            timestamp_dt = datetime.fromisoformat(timestamp).replace(tzinfo=timezone.utc)\n",
    "            now = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "\n",
    "            if (now - timestamp_dt) >= timedelta(days=self.cache_ttl_days):\n",
    "                self.logger.info(\"Cache expired. Fetching new data.\")\n",
    "                return []\n",
    "\n",
    "            return cache_data['papers']\n",
    "\n",
    "        except (json.JSONDecodeError, IOError) as e:\n",
    "            self.logger.error(f\"Cache read error: {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    def _save_to_cache(self, query: str, source: str, papers: list) -> None:\n",
    "        \"\"\"Save results to cache.\"\"\"\n",
    "        if not papers:\n",
    "            return\n",
    "\n",
    "        cache_file = self.cache_dir / f\"{self._get_cache_key(query, source)}.json\"\n",
    "        \n",
    "        try:\n",
    "            cache_data = {\n",
    "                'timestamp': datetime.utcnow().replace(tzinfo=timezone.utc).isoformat(),\n",
    "                'papers': papers,\n",
    "                'query': query,\n",
    "                'source': source\n",
    "            }\n",
    "            \n",
    "            # Write to temporary file first for atomic operation\n",
    "            temp_file = cache_file.with_suffix('.tmp')\n",
    "            with temp_file.open('w', encoding=\"utf-8\") as f:\n",
    "                json.dump(cache_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            # Atomic rename\n",
    "            temp_file.replace(cache_file)\n",
    "            \n",
    "        except IOError as e:\n",
    "            self.logger.error(f\"Cache write error: {e}\", exc_info=True)\n",
    "\n",
    "    def _rate_limit(self) -> None:\n",
    "        \"\"\"Enforce rate limiting between API requests.\"\"\"\n",
    "        now = time.monotonic()\n",
    "        time_since_last_request = now - self.last_request_time\n",
    "\n",
    "        if time_since_last_request < self.request_delay:\n",
    "            sleep_time = self.request_delay - time_since_last_request\n",
    "            self.logger.debug(f\"Rate limiting: Sleeping for {sleep_time:.2f} seconds\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        self.last_request_time = time.monotonic()\n",
    "\n",
    "    async def process_papers(self, papers: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Process and validate collected papers.\"\"\"\n",
    "        processed_papers = []\n",
    "        for paper in papers:\n",
    "            if self._validate_paper(paper):\n",
    "                processed_papers.append(paper)\n",
    "        return processed_papers\n",
    "\n",
    "    def _validate_paper(self, paper: Dict) -> bool:\n",
    "        \"\"\"Validate paper data structure.\"\"\"\n",
    "        required_fields = ['title', 'authors', 'source']\n",
    "        return all(field in paper for field in required_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Cell 4: Enhanced Paper Fetcher with Vector Storage\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain_community.utilities.arxiv import ArxivAPIWrapper\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.schema import Document\n",
    "\n",
    "class PaperFetcher(ResearchCollector):\n",
    "    \"\"\"Enhanced system for fetching and storing academic papers with vector integration.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize session and API wrappers\n",
    "        self.session = requests.Session()\n",
    "        self.arxiv_wrapper = ArxivAPIWrapper(\n",
    "            top_k_results=100,\n",
    "            load_max_docs=100\n",
    "        )\n",
    "        \n",
    "        # Configure API settings\n",
    "        self.semantic_scholar_config = {\n",
    "            \"base_url\": \"https://api.semanticscholar.org/graph/v1\",\n",
    "            \"fields\": \"title,abstract,authors,year,venue,citationCount,url,externalIds\",\n",
    "            \"headers\": {\n",
    "                \"Accept\": \"application/json\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Initialize vector store\n",
    "        self.vector_dir = Path(\"vector_store\")\n",
    "        self.vector_dir.mkdir(exist_ok=True)\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.vector_store = Chroma(\n",
    "            persist_directory=str(self.vector_dir),\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "\n",
    "    async def fetch_papers(self, query: str, max_results: int = 50) -> List[ResearchPaper]:\n",
    "        \"\"\"Coordinate paper collection and storage from multiple sources.\"\"\"\n",
    "        self.logger.info(f\"Beginning paper collection for query: {query}\")\n",
    "        \n",
    "        papers = []\n",
    "        cached_papers = self._check_cache(query, \"combined\")\n",
    "        if cached_papers:\n",
    "            papers = [ResearchPaper(**p) for p in cached_papers]\n",
    "            self.logger.info(\"Retrieved papers from cache\")\n",
    "        else:\n",
    "            try:\n",
    "                semantic_papers = await self.fetch_from_semantic_scholar(query, max_results)\n",
    "                papers.extend(semantic_papers)\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                if e.response.status_code == 429:\n",
    "                    self.logger.info(\"Semantic Scholar rate limited, proceeding with ArXiv only\")\n",
    "                else:\n",
    "                    self.logger.warning(f\"Semantic Scholar search failed: {str(e)}\")\n",
    "            try:\n",
    "                arxiv_papers = await self.fetch_from_arxiv(query, max_results)\n",
    "                papers.extend(arxiv_papers)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"ArXiv search failed: {str(e)}\")\n",
    "\n",
    "            if papers:\n",
    "                await self._store_papers_in_vector_db(papers)\n",
    "                self._save_to_cache(query, \"combined\", [p.model_dump() for p in papers])\n",
    "        \n",
    "        return papers[:max_results]\n",
    "\n",
    "    async def fetch_from_semantic_scholar(self, query: str, limit: int) -> List[ResearchPaper]:\n",
    "        \"\"\"Fetch papers from Semantic Scholar with proper API formatting.\"\"\"\n",
    "        search_url = f\"{self.semantic_scholar_config['base_url']}/paper/search\"\n",
    "        \n",
    "        params = {\n",
    "            \"query\": query,\n",
    "            \"limit\": min(limit, 100),\n",
    "            \"fields\": self.semantic_scholar_config[\"fields\"]\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            self._rate_limit()\n",
    "            response = self.session.get(\n",
    "                search_url,\n",
    "                params=params,\n",
    "                headers=self.semantic_scholar_config[\"headers\"]\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            papers = []\n",
    "            \n",
    "            for item in data.get(\"data\", []):\n",
    "                try:\n",
    "                    paper = self._parse_semantic_scholar_paper(item)\n",
    "                    if paper:\n",
    "                        papers.append(paper)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error parsing paper data: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "            return papers[:limit]\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code == 429:\n",
    "                self.logger.warning(\"Semantic Scholar rate limit reached\")\n",
    "            raise\n",
    "\n",
    "    async def fetch_from_arxiv(self, query: str, limit: int) -> List[ResearchPaper]:\n",
    "        \"\"\"Fetch papers from ArXiv with proper rate limiting.\"\"\"\n",
    "        self.logger.info(f\"Fetching papers from ArXiv: {query}\")\n",
    "\n",
    "        try:\n",
    "            self._rate_limit()\n",
    "            raw_results = self.arxiv_wrapper.run(query)\n",
    "            papers = []\n",
    "            \n",
    "            entries = [e for e in raw_results.split(\"\\n\\n\") if e.strip()]\n",
    "            for entry in entries[:limit]:\n",
    "                try:\n",
    "                    paper_data = self._parse_arxiv_entry(entry)\n",
    "                    if paper_data:\n",
    "                        paper = self._create_paper_from_arxiv(paper_data)\n",
    "                        if paper:\n",
    "                            papers.append(paper)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing ArXiv entry: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            return papers\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ArXiv API error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def _store_papers_in_vector_db(self, papers: List[ResearchPaper]) -> None:\n",
    "        \"\"\"Store papers in vector database with enhanced metadata.\"\"\"\n",
    "        from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "        \n",
    "        documents = []\n",
    "        for paper in papers:\n",
    "            content = self._prepare_paper_content(paper)\n",
    "            # Convert all metadata values to basic types and filter out None values\n",
    "            raw_metadata = {\n",
    "                \"title\": str(paper.title),\n",
    "                \"year\": int(paper.year) if paper.year else 0,\n",
    "                \"venue\": str(paper.venue) if paper.venue else \"\",\n",
    "                \"citation_count\": int(paper.citation_count),\n",
    "                \"source\": str(paper.source),\n",
    "                \"url\": str(paper.url) if paper.url else \"\",\n",
    "                \"doi\": str(paper.doi) if paper.doi else \"\",\n",
    "                \"authors\": \", \".join(str(a.name) for a in paper.authors)\n",
    "            }\n",
    "            # Filter out any remaining complex types\n",
    "            metadata = filter_complex_metadata(raw_metadata)\n",
    "            documents.append(Document(page_content=content, metadata=metadata))\n",
    "        \n",
    "        try:\n",
    "            self.vector_store.add_documents(documents)\n",
    "            self.vector_store.persist()\n",
    "            self.logger.info(f\"Stored {len(papers)} papers in vector database\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error storing papers in vector database: {str(e)}\")\n",
    "            # Continue execution even if vector storage fails\n",
    "            pass\n",
    "\n",
    "    def _prepare_paper_content(self, paper: ResearchPaper) -> str:\n",
    "        \"\"\"Prepare paper content for vector storage.\"\"\"\n",
    "        content_parts = [\n",
    "            f\"Title: {paper.title}\",\n",
    "            f\"Abstract: {paper.abstract or ''}\",\n",
    "            f\"Authors: {', '.join(a.name for a in paper.authors)}\",\n",
    "            f\"Venue: {paper.venue or ''}\"\n",
    "        ]\n",
    "        return \"\\n\".join(content_parts)\n",
    "\n",
    "    def _prepare_paper_metadata(self, paper: ResearchPaper) -> Dict:\n",
    "        \"\"\"Prepare paper metadata for vector storage, ensuring all values are simple types.\"\"\"\n",
    "        metadata = {\n",
    "            \"title\": str(paper.title) if paper.title else \"\",\n",
    "            \"year\": int(paper.year) if paper.year else 0,\n",
    "            \"venue\": str(paper.venue) if paper.venue else \"\",\n",
    "            \"citation_count\": int(paper.citation_count) if paper.citation_count is not None else 0,\n",
    "            \"source\": str(paper.source) if paper.source else \"\",\n",
    "            \"url\": str(paper.url) if paper.url else \"\",\n",
    "            \"doi\": str(paper.doi) if paper.doi else \"\"\n",
    "        }\n",
    "        \n",
    "        # Convert author list to string to avoid complex types\n",
    "        metadata[\"authors\"] = \", \".join(a.name for a in paper.authors) if paper.authors else \"\"\n",
    "        \n",
    "        # Remove any remaining None values\n",
    "        metadata = {k: v for k, v in metadata.items() if v is not None}\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "    def _parse_semantic_scholar_paper(self, data: Dict) -> Optional[ResearchPaper]:\n",
    "        \"\"\"Parse Semantic Scholar API response into ResearchPaper model.\"\"\"\n",
    "        try:\n",
    "            return ResearchPaper(\n",
    "                title=data[\"title\"],\n",
    "                authors=[\n",
    "                    Author(\n",
    "                        name=author.get(\"name\", \"Unknown\"),\n",
    "                        affiliations=[]\n",
    "                    ) for author in data.get(\"authors\", [])\n",
    "                ],\n",
    "                year=data.get(\"year\"),\n",
    "                abstract=data.get(\"abstract\"),\n",
    "                doi=data.get(\"externalIds\", {}).get(\"DOI\"),\n",
    "                url=data.get(\"url\"),\n",
    "                venue=data.get(\"venue\"),\n",
    "                citation_count=data.get(\"citationCount\", 0),\n",
    "                source=\"Semantic Scholar\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to parse paper: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _parse_arxiv_entry(self, entry: str) -> Dict[str, Any]:\n",
    "        \"\"\"Parse ArXiv entry text into structured data.\"\"\"\n",
    "        data = {}\n",
    "        current_field = None\n",
    "        current_content = []\n",
    "\n",
    "        for line in entry.split('\\n'):\n",
    "            if ':' in line and not line.startswith(' '):\n",
    "                if current_field:\n",
    "                    data[current_field] = ' '.join(current_content).strip()\n",
    "                    current_content = []\n",
    "                field, content = line.split(':', 1)\n",
    "                current_field = field.strip().lower()\n",
    "                current_content.append(content.strip())\n",
    "            elif current_field:\n",
    "                current_content.append(line.strip())\n",
    "\n",
    "        if current_field and current_content:\n",
    "            data[current_field] = ' '.join(current_content).strip()\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def _create_paper_from_arxiv(self, paper_data: Dict) -> Optional[ResearchPaper]:\n",
    "        \"\"\"Create a ResearchPaper object from ArXiv paper data.\"\"\"\n",
    "        try:\n",
    "            authors = [\n",
    "                Author(name=name.strip()) \n",
    "                for name in paper_data.get(\"authors\", \"\").split(\",\")\n",
    "                if name.strip()\n",
    "            ]\n",
    "            \n",
    "            year = None\n",
    "            if paper_data.get(\"published\"):\n",
    "                try:\n",
    "                    year = int(paper_data[\"published\"][:4])\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "            return ResearchPaper(\n",
    "                title=paper_data.get(\"title\", \"\").strip(),\n",
    "                authors=authors,\n",
    "                year=year,\n",
    "                abstract=paper_data.get(\"summary\", \"\").strip(),\n",
    "                url=paper_data.get(\"entry_id\"),\n",
    "                venue=\"arXiv\",\n",
    "                citation_count=0,\n",
    "                source=\"ArXiv\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create paper from ArXiv data: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Cell 5: Research Paper Analysis System\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "class ResearchAnalyzer:\n",
    "    \"\"\"Advanced system for analyzing and ranking academic research papers.\"\"\"\n",
    "\n",
    "    def __init__(self, paper_fetcher: PaperFetcher):\n",
    "        self.paper_fetcher = paper_fetcher\n",
    "        self.logger = logging.getLogger(__name__ + \".ResearchAnalyzer\")\n",
    "        \n",
    "        # Configure analysis weights\n",
    "        self.quality_weights = {\n",
    "            \"relevance\": 0.35,\n",
    "            \"citation_impact\": 0.25,\n",
    "            \"methodology\": 0.20,\n",
    "            \"recency\": 0.10,\n",
    "            \"venue_quality\": 0.10\n",
    "        }\n",
    "\n",
    "    async def analyze_research_topic(self, query: str, max_results: int = 20) -> Dict:\n",
    "        \"\"\"Perform comprehensive analysis of papers for a research topic.\"\"\"\n",
    "        self.logger.info(f\"Beginning analysis for query: {query}\")\n",
    "\n",
    "        try:\n",
    "            papers = await self.paper_fetcher.fetch_papers(query, max_results)\n",
    "            if not papers:\n",
    "                return {\"status\": \"error\", \"message\": \"No papers found for the given query\"}\n",
    "\n",
    "            # Perform similarity search using vector store\n",
    "            vector_results = self.paper_fetcher.vector_store.similarity_search_with_scores(\n",
    "                query, k=max_results\n",
    "            )\n",
    "\n",
    "            analyzed_papers = []\n",
    "            for doc, similarity_score in vector_results:\n",
    "                paper_analysis = self._analyze_single_paper(doc, similarity_score)\n",
    "                if paper_analysis:\n",
    "                    analyzed_papers.append(paper_analysis)\n",
    "\n",
    "            ranked_papers = sorted(\n",
    "                analyzed_papers,\n",
    "                key=lambda x: x[\"scores\"][\"overall\"],\n",
    "                reverse=True\n",
    "            )\n",
    "\n",
    "            analysis_summary = self._generate_research_summary(ranked_papers)\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"query\": query,\n",
    "                \"summary\": analysis_summary,\n",
    "                \"papers\": ranked_papers\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Analysis failed: {str(e)}\", exc_info=True)\n",
    "            return {\"status\": \"error\", \"message\": f\"Analysis failed: {str(e)}\"}\n",
    "\n",
    "    def _analyze_single_paper(self, paper: Document, similarity_score: float) -> Dict:\n",
    "        \"\"\"Analyze a single research paper across multiple dimensions.\"\"\"\n",
    "        try:\n",
    "            metadata = paper.metadata\n",
    "            current_year = datetime.now().year\n",
    "\n",
    "            citation_impact = self._calculate_citation_impact(\n",
    "                metadata.get(\"citation_count\", 0),\n",
    "                metadata.get(\"year\", current_year)\n",
    "            )\n",
    "\n",
    "            methodology_score = self._assess_methodology(paper.page_content)\n",
    "            recency_score = self._calculate_recency_score(metadata.get(\"year\", current_year))\n",
    "            venue_score = self._assess_venue_quality(metadata.get(\"venue\"), metadata.get(\"source\"))\n",
    "\n",
    "            overall_score = sum([\n",
    "                self.quality_weights[\"relevance\"] * similarity_score,\n",
    "                self.quality_weights[\"citation_impact\"] * citation_impact,\n",
    "                self.quality_weights[\"methodology\"] * methodology_score,\n",
    "                self.quality_weights[\"recency\"] * recency_score,\n",
    "                self.quality_weights[\"venue_quality\"] * venue_score\n",
    "            ])\n",
    "\n",
    "            return {\n",
    "                \"title\": metadata.get(\"title\"),\n",
    "                \"year\": metadata.get(\"year\"),\n",
    "                \"authors\": metadata.get(\"authors\", []),\n",
    "                \"url\": metadata.get(\"url\"),\n",
    "                \"source\": metadata.get(\"source\"),\n",
    "                \"venue\": metadata.get(\"venue\"),\n",
    "                \"scores\": {\n",
    "                    \"relevance\": similarity_score,\n",
    "                    \"citation_impact\": citation_impact,\n",
    "                    \"methodology\": methodology_score,\n",
    "                    \"recency\": recency_score,\n",
    "                    \"venue_quality\": venue_score,\n",
    "                    \"overall\": overall_score\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error analyzing paper: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _calculate_citation_impact(self, citations: int, year: int) -> float:\n",
    "        \"\"\"Calculate normalized citation impact score.\"\"\"\n",
    "        years_since_publication = max(1, datetime.now().year - year)\n",
    "        citations_per_year = citations / years_since_publication\n",
    "        return min(1.0, np.log1p(citations_per_year) / np.log1p(100))\n",
    "\n",
    "    def _assess_methodology(self, content: str) -> float:\n",
    "        \"\"\"Assess research methodology quality based on content analysis.\"\"\"\n",
    "        methodology_indicators = {\n",
    "            \"methodology\": 1.0,\n",
    "            \"experiment\": 0.8,\n",
    "            \"statistical analysis\": 0.8,\n",
    "            \"data collection\": 0.7,\n",
    "            \"sample size\": 0.7,\n",
    "            \"control group\": 0.9,\n",
    "            \"randomized\": 0.9,\n",
    "            \"validation\": 0.8\n",
    "        }\n",
    "        \n",
    "        content_lower = content.lower()\n",
    "        scored_indicators = [\n",
    "            weight for indicator, weight in methodology_indicators.items()\n",
    "            if indicator in content_lower\n",
    "        ]\n",
    "        \n",
    "        return sum(scored_indicators) / len(methodology_indicators) if scored_indicators else 0.5\n",
    "\n",
    "    def _calculate_recency_score(self, year: int) -> float:\n",
    "        \"\"\"Calculate recency score with exponential decay.\"\"\"\n",
    "        years_old = max(0, datetime.now().year - year)\n",
    "        return np.exp(-0.2 * years_old)\n",
    "\n",
    "    def _assess_venue_quality(self, venue: str, source: str) -> float:\n",
    "        \"\"\"Assess the quality of the publication venue.\"\"\"\n",
    "        if not venue:\n",
    "            return 0.5\n",
    "\n",
    "        venue_lower = venue.lower() if venue else \"\"\n",
    "        source_lower = source.lower() if source else \"\"\n",
    "\n",
    "        # High-impact venue scoring\n",
    "        if any(journal in venue_lower for journal in [\"nature\", \"science\", \"cell\"]):\n",
    "            return 1.0\n",
    "        elif \"journal\" in venue_lower:\n",
    "            return 0.8\n",
    "        elif \"conference\" in venue_lower:\n",
    "            return 0.75\n",
    "        elif \"arxiv\" in source_lower:\n",
    "            return 0.7\n",
    "        else:\n",
    "            return 0.6\n",
    "\n",
    "    def _generate_research_summary(self, analyzed_papers: List[Dict]) -> Dict:\n",
    "        \"\"\"Generate a comprehensive summary of the research analysis.\"\"\"\n",
    "        if not analyzed_papers:\n",
    "            return {}\n",
    "\n",
    "        year_distribution = defaultdict(int)\n",
    "        source_distribution = defaultdict(int)\n",
    "        avg_scores = defaultdict(float)\n",
    "\n",
    "        for paper in analyzed_papers:\n",
    "            year_distribution[paper[\"year\"]] += 1\n",
    "            source_distribution[paper[\"source\"]] += 1\n",
    "            \n",
    "            for score_type, score in paper[\"scores\"].items():\n",
    "                avg_scores[score_type] += score\n",
    "\n",
    "        total_papers = len(analyzed_papers)\n",
    "        for score_type in avg_scores:\n",
    "            avg_scores[score_type] /= total_papers\n",
    "\n",
    "        return {\n",
    "            \"total_papers\": total_papers,\n",
    "            \"year_range\": {\n",
    "                \"oldest\": min(year_distribution.keys()),\n",
    "                \"newest\": max(year_distribution.keys())\n",
    "            },\n",
    "            \"source_distribution\": dict(source_distribution),\n",
    "            \"average_scores\": dict(avg_scores),\n",
    "            \"top_venues\": [\n",
    "                paper[\"venue\"] for paper in analyzed_papers[:3]\n",
    "                if paper[\"venue\"]\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninterface = ResearchInterface()\\nresults = await interface.search_and_analyze()\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% Cell 6: Research Interface and Analysis Display\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "\n",
    "class ResearchInterface:\n",
    "    \"\"\"Interface for collecting and analyzing research papers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.paper_fetcher = PaperFetcher()\n",
    "        self.analyzer = ResearchAnalyzer(self.paper_fetcher)\n",
    "        self.logger = logging.getLogger(__name__ + \".ResearchInterface\")\n",
    "\n",
    "    async def search_and_analyze(self):\n",
    "        \"\"\"Interactive research paper search and analysis.\"\"\"\n",
    "        print(\"\\nAdvanced Research Paper Analysis\")\n",
    "        print(\"===============================\")\n",
    "        \n",
    "        query = input(\"\\nEnter your research query (use 'and' to combine specific concepts): \")\n",
    "        max_results = input(\"Enter maximum number of papers to analyze (default 30): \")\n",
    "        max_results = int(max_results) if max_results.isdigit() else 30\n",
    "\n",
    "        print(\"\\nCollecting and analyzing papers...\")\n",
    "        analysis_results = await self.analyzer.analyze_research_topic(query, max_results)\n",
    "\n",
    "        if analysis_results[\"status\"] == \"success\":\n",
    "            self._display_analysis_results(analysis_results)\n",
    "            return analysis_results\n",
    "        else:\n",
    "            print(f\"\\nError: {analysis_results['message']}\")\n",
    "            return None\n",
    "\n",
    "    def _display_analysis_results(self, results):\n",
    "        \"\"\"Display analyzed papers in a structured format.\"\"\"\n",
    "        summary = results[\"summary\"]\n",
    "        papers = results[\"papers\"]\n",
    "\n",
    "        # Display summary statistics\n",
    "        print(\"\\nAnalysis Summary\")\n",
    "        print(\"--------------\")\n",
    "        print(f\"Total Papers Analyzed: {summary['total_papers']}\")\n",
    "        print(f\"Year Range: {summary['year_range']['oldest']} - {summary['year_range']['newest']}\")\n",
    "        print(\"\\nSource Distribution:\")\n",
    "        for source, count in summary['source_distribution'].items():\n",
    "            print(f\"  {source}: {count} papers\")\n",
    "\n",
    "        # Create DataFrame for detailed paper analysis\n",
    "        paper_data = []\n",
    "        for paper in papers:\n",
    "            paper_data.append({\n",
    "                \"Title\": paper[\"title\"],\n",
    "                \"Year\": paper[\"year\"],\n",
    "                \"Authors\": \"; \".join(paper[\"authors\"]),\n",
    "                \"Venue\": paper[\"venue\"] or \"N/A\",\n",
    "                \"Overall Score\": f\"{paper['scores']['overall']:.3f}\",\n",
    "                \"Relevance\": f\"{paper['scores']['relevance']:.3f}\",\n",
    "                \"Citation Impact\": f\"{paper['scores']['citation_impact']:.3f}\",\n",
    "                \"Methodology\": f\"{paper['scores']['methodology']:.3f}\",\n",
    "                \"URL\": paper[\"url\"] or \"N/A\"\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(paper_data)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\nRanked Papers (sorted by overall score)\")\n",
    "        print(\"-------------------------------------\")\n",
    "        \n",
    "        # Configure pandas display options\n",
    "        pd.set_option('display.max_colwidth', None)\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        \n",
    "        # Create styled DataFrame\n",
    "        styled_df = df.style.background_gradient(subset=['Overall Score'], cmap='YlOrRd')\n",
    "        display(HTML(styled_df.to_html(escape=False)))\n",
    "\n",
    "        return df\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "interface = ResearchInterface()\n",
    "results = await interface.search_and_analyze()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 7\n",
    "- User interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 14:51:46,927 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advanced Research Paper Analysis\n",
      "===============================\n",
      "\n",
      "Collecting and analyzing papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Semantic Scholar rate limit reached\n",
      "2025-02-12 14:51:56,863 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=Mitophagy+and+mitochondrial+quality+control&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "2025-02-12 14:51:57,620 - INFO - Got first page: 20 of 2595038 total results\n",
      "2025-02-12 14:51:57,622 - INFO - Sleeping: 2.968749 seconds\n",
      "2025-02-12 14:52:00,597 - INFO - Requesting page (first: False, try: 0): https://export.arxiv.org/api/query?search_query=Mitophagy+and+mitochondrial+quality+control&id_list=&sortBy=relevance&sortOrder=descending&start=20&max_results=100\n",
      "Analysis failed: 'str' object has no attribute 'metadata'\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/j7/smpqy2fn30l7j5kcqh76jqp40000gn/T/ipykernel_5521/2691737644.py\", line 29, in analyze_research_topic\n",
      "    papers = await self.paper_fetcher.fetch_papers(query, max_results)\n",
      "  File \"/var/folders/j7/smpqy2fn30l7j5kcqh76jqp40000gn/T/ipykernel_5521/1005013943.py\", line 67, in fetch_papers\n",
      "    await self._store_papers_in_vector_db(papers)\n",
      "  File \"/var/folders/j7/smpqy2fn30l7j5kcqh76jqp40000gn/T/ipykernel_5521/1005013943.py\", line 156, in _store_papers_in_vector_db\n",
      "    metadata = filter_complex_metadata(raw_metadata)\n",
      "  File \"/Users/davidburton/miniforge3/envs/article_env/lib/python3.10/site-packages/langchain_community/vectorstores/utils.py\", line 66, in filter_complex_metadata\n",
      "    for key, value in document.metadata.items():\n",
      "AttributeError: 'str' object has no attribute 'metadata'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error: Analysis failed: 'str' object has no attribute 'metadata'\n"
     ]
    }
   ],
   "source": [
    "interface = ResearchInterface()\n",
    "results = await interface.search_and_analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (article_env)",
   "language": "python",
   "name": "article_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
