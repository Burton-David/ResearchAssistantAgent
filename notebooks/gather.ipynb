{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 1\n",
    "* Imports and environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Cell 1: Initial Setup and Essential Imports\n",
    "import os\n",
    "import logging\n",
    "import asyncio\n",
    "import json\n",
    "import nest_asyncio\n",
    "from datetime import datetime as dt\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, HttpUrl\n",
    "\n",
    "# Load environment variables (API keys, etc.)\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configure logging\n",
    "LOG_FILE = 'research_collector.log'\n",
    "LOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=LOG_FORMAT,\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_FILE),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.propagate = False  # Prevent duplicate logging\n",
    "\n",
    "# Load environment variables\n",
    "# Load environment variables\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path(\"/Users/davidburton/src/research_paper_analysis\")\n",
    "ENV_PATH = PROJECT_ROOT / \"config\" / \".env\"\n",
    "load_dotenv(ENV_PATH)\n",
    "\n",
    "# Verify environment variables\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    logger.info(\"OpenAI API key loaded successfully\")\n",
    "else:\n",
    "    logger.warning(\"OpenAI API key not found in environment variables\")\n",
    "    \n",
    "# Apply nest_asyncio only in Jupyter environments\n",
    "try:\n",
    "    get_ipython  # Check if running in Jupyter\n",
    "    nest_asyncio.apply()\n",
    "    logger.info(\"Applied nest_asyncio for async support in Jupyter.\")\n",
    "except NameError:\n",
    "    pass  # Skip in standard Python scripts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 2\n",
    "* Author\n",
    "* ResearchPaper\n",
    "  * get_embedding_content\n",
    "  * to_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Cell 2: Core Data Models\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field, HttpUrl\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "class Author(BaseModel):\n",
    "    \"\"\"Author information model\"\"\"\n",
    "    name: str\n",
    "    affiliations: List[str] = Field(default_factory=list)\n",
    "\n",
    "class ResearchPaper(BaseModel):\n",
    "    \"\"\"Core research paper model with vector store compatibility\"\"\"\n",
    "    id: str = Field(default=\"\")\n",
    "    title: str\n",
    "    authors: List[Author]\n",
    "    year: Optional[int] = None\n",
    "    abstract: Optional[str] = None\n",
    "    doi: Optional[str] = None\n",
    "    url: Optional[HttpUrl] = None\n",
    "    venue: Optional[str] = None\n",
    "    citation_count: int = Field(default=0)\n",
    "    source: str\n",
    "    metadata: Dict = Field(default_factory=dict)\n",
    "    \n",
    "    def get_embedding_content(self) -> str:\n",
    "        \"\"\"Generate content for vector store embedding\"\"\"\n",
    "        content_parts = [\n",
    "            self.title,\n",
    "            self.abstract or \"\",\n",
    "            \" \".join(a.name for a in self.authors),\n",
    "            self.venue or \"\"\n",
    "        ]\n",
    "        return \"\\n\".join(filter(None, content_parts))\n",
    "    \n",
    "    def to_document(self) -> Dict:\n",
    "        \"\"\"Convert to vector store compatible document\"\"\"\n",
    "        return {\n",
    "            \"page_content\": self.get_embedding_content(),\n",
    "            \"metadata\": {\n",
    "                \"id\": self.id,\n",
    "                \"title\": self.title,\n",
    "                \"year\": self.year or 0,\n",
    "                \"venue\": self.venue or \"\",\n",
    "                \"citation_count\": self.citation_count,\n",
    "                \"source\": self.source,\n",
    "                \"url\": str(self.url) if self.url else \"\",\n",
    "                \"doi\": self.doi or \"\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3\n",
    "* PaperCollector\n",
    "  * fetch_papers\n",
    "  * _process_semantic_scholar_papers\n",
    "  * _fetch_arxiv_papers\n",
    "  * _parse_arxiv_entry\n",
    "  * _store_in_vector_db\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Cell 3: Paper Collection System\n",
    "from scholarly import scholarly\n",
    "from semanticscholar import SemanticScholar\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_community.utilities.arxiv import ArxivAPIWrapper\n",
    "import asyncio\n",
    "from typing import List, Dict, Any\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "import time\n",
    "\n",
    "class PaperCollector:\n",
    "    def __init__(self):\n",
    "        self.semantic_scholar = SemanticScholar()\n",
    "        self.arxiv_wrapper = ArxivAPIWrapper(\n",
    "            top_k_results=100,\n",
    "            load_max_docs=100\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.vector_store = Chroma(\n",
    "            persist_directory=\"vector_store\",\n",
    "            embedding_function=OpenAIEmbeddings()\n",
    "        )\n",
    "        # Add rate limiting parameters\n",
    "        self.min_request_interval = 2.0  # Minimum seconds between requests\n",
    "        self.last_request_time = 0\n",
    "\n",
    "    def _rate_limit(self):\n",
    "        \"\"\"Ensure minimum time between requests\"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last_request = current_time - self.last_request_time\n",
    "        if time_since_last_request < self.min_request_interval:\n",
    "            sleep_time = self.min_request_interval - time_since_last_request\n",
    "            time.sleep(sleep_time)\n",
    "        self.last_request_time = time.time()    \n",
    "        \n",
    "    async def fetch_papers(self, query: str, max_results: int = 30) -> List[ResearchPaper]:\n",
    "        \"\"\"Fetch papers from multiple sources\"\"\"\n",
    "        papers = []\n",
    "        \n",
    "        try:\n",
    "            # Fetch from Semantic Scholar\n",
    "            try:\n",
    "                sem_papers = self.semantic_scholar.search_paper(query, limit=max_results)\n",
    "                papers.extend(self._process_semantic_scholar_papers(sem_papers))\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error fetching from Semantic Scholar: {e}\")\n",
    "            \n",
    "            # Fetch from arXiv\n",
    "            try:\n",
    "                arxiv_papers = await self._fetch_arxiv_papers(query, max_results)\n",
    "                papers.extend(arxiv_papers)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error fetching from arXiv: {e}\")\n",
    "            \n",
    "            # Store in vector database if we got any papers\n",
    "            if papers:\n",
    "                await self._store_in_vector_db(papers)\n",
    "            \n",
    "            return papers[:max_results]  # Return only up to max_results papers\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in paper collection: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _process_semantic_scholar_papers(self, papers: List[Any]) -> List[ResearchPaper]:\n",
    "            \"\"\"Process Semantic Scholar papers into our model\"\"\"\n",
    "            processed = []\n",
    "            for paper in papers:\n",
    "                try:\n",
    "                    # Get DOI safely with fallback\n",
    "                    doi = None\n",
    "                    if hasattr(paper, 'externalIds'):\n",
    "                        doi = paper.externalIds.get('DOI')\n",
    "                    \n",
    "                    processed.append(\n",
    "                        ResearchPaper(\n",
    "                            id=paper.paperId,\n",
    "                            title=paper.title,\n",
    "                            authors=[Author(name=author.name) for author in paper.authors],\n",
    "                            year=paper.year,\n",
    "                            abstract=paper.abstract,\n",
    "                            doi=doi,  # This can now be None\n",
    "                            url=paper.url,\n",
    "                            venue=paper.venue,\n",
    "                            citation_count=paper.citationCount,\n",
    "                            source=\"Semantic Scholar\"\n",
    "                        )\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing paper {paper.title}: {e}\")\n",
    "                    continue\n",
    "            return processed\n",
    "    \n",
    "    async def _fetch_arxiv_papers(self, query: str, limit: int) -> List[ResearchPaper]:\n",
    "        \"\"\"Fetch papers from arXiv\"\"\"\n",
    "        processed = []\n",
    "        try:\n",
    "            raw_results = self.arxiv_wrapper.run(query)\n",
    "            entries = [e for e in raw_results.split(\"\\n\\n\") if e.strip()]\n",
    "            \n",
    "            for entry in entries[:limit]:\n",
    "                try:\n",
    "                    paper_data = self._parse_arxiv_entry(entry)\n",
    "                    if paper_data:\n",
    "                        # Generate a unique ID for arXiv papers\n",
    "                        paper_id = f\"arxiv_{hash(paper_data.get('entry_id', ''))}\"\n",
    "                        \n",
    "                        # Extract year from published date\n",
    "                        year = None\n",
    "                        if paper_data.get(\"published\"):\n",
    "                            try:\n",
    "                                year = int(paper_data[\"published\"][:4])\n",
    "                            except ValueError:\n",
    "                                pass\n",
    "                        \n",
    "                        processed.append(\n",
    "                            ResearchPaper(\n",
    "                                id=paper_id,\n",
    "                                title=paper_data.get(\"title\", \"\").strip(),\n",
    "                                authors=[\n",
    "                                    Author(name=name.strip()) \n",
    "                                    for name in paper_data.get(\"authors\", \"\").split(\",\")\n",
    "                                    if name.strip()\n",
    "                                ],\n",
    "                                year=year,\n",
    "                                abstract=paper_data.get(\"summary\", \"\").strip(),\n",
    "                                url=paper_data.get(\"entry_id\"),\n",
    "                                venue=\"arXiv\",\n",
    "                                citation_count=0,  # arXiv doesn't provide citation counts\n",
    "                                source=\"arXiv\"\n",
    "                            )\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing arXiv entry: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in arXiv fetching: {e}\")\n",
    "            \n",
    "        return processed\n",
    "    \n",
    "    def _parse_arxiv_entry(self, entry: str) -> Dict[str, Any]:\n",
    "        \"\"\"Parse arXiv entry text into structured data\"\"\"\n",
    "        data = {}\n",
    "        current_field = None\n",
    "        current_content = []\n",
    "\n",
    "        for line in entry.split('\\n'):\n",
    "            if ':' in line and not line.startswith(' '):\n",
    "                if current_field:\n",
    "                    data[current_field] = ' '.join(current_content).strip()\n",
    "                    current_content = []\n",
    "                field, content = line.split(':', 1)\n",
    "                current_field = field.strip().lower()\n",
    "                current_content.append(content.strip())\n",
    "            elif current_field:\n",
    "                current_content.append(line.strip())\n",
    "\n",
    "        if current_field and current_content:\n",
    "            data[current_field] = ' '.join(current_content).strip()\n",
    "\n",
    "        return data\n",
    "    \n",
    "    async def _store_in_vector_db(self, papers: List[ResearchPaper]) -> None:\n",
    "        \"\"\"Store papers in vector database\"\"\"\n",
    "        try:\n",
    "            documents = [\n",
    "                Document(**paper.to_document())\n",
    "                for paper in papers\n",
    "            ]\n",
    "            filtered_docs = filter_complex_metadata(documents)\n",
    "            self.vector_store.add_documents(filtered_docs)\n",
    "            self.vector_store.persist()\n",
    "            self.logger.info(f\"Stored {len(papers)} papers in vector database\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error storing in vector database: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4\n",
    "* analyze_research_topic\n",
    "* _analyze_single_paper\n",
    "* _calculate_citation_impact\n",
    "* _assess_methodology\n",
    "* _calculate_recency_score\n",
    "* _assess_venue_quality\n",
    "* _generate_research_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Cell 4: Research Analysis System\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class ResearchAnalyzer:\n",
    "    \"\"\"Advanced system for analyzing and ranking academic research papers.\"\"\"\n",
    "\n",
    "    def __init__(self, paper_collector: PaperCollector):\n",
    "        self.paper_collector = paper_collector\n",
    "        self.logger = logging.getLogger(__name__ + \".ResearchAnalyzer\")\n",
    "        \n",
    "        # Configure analysis weights\n",
    "        self.quality_weights = {\n",
    "            \"relevance\": 0.35,\n",
    "            \"citation_impact\": 0.25,\n",
    "            \"methodology\": 0.20,\n",
    "            \"recency\": 0.10,\n",
    "            \"venue_quality\": 0.10\n",
    "        }\n",
    "\n",
    "    async def analyze_research_topic(self, query: str, max_results: int = 20) -> Dict:\n",
    "        \"\"\"Perform comprehensive analysis of papers for a research topic.\"\"\"\n",
    "        self.logger.info(f\"Beginning analysis for query: {query}\")\n",
    "\n",
    "        try:\n",
    "            # Fetch papers using our new collector\n",
    "            papers = await self.paper_collector.fetch_papers(query, max_results)\n",
    "            if not papers:\n",
    "                return {\"status\": \"error\", \"message\": \"No papers found for the given query\"}\n",
    "\n",
    "            # Get similar papers from vector store\n",
    "            vector_results = self.paper_collector.vector_store.similarity_search_with_scores(\n",
    "                query, k=max_results\n",
    "            )\n",
    "\n",
    "            analyzed_papers = []\n",
    "            for paper, similarity_score in zip(papers, [score for _, score in vector_results]):\n",
    "                paper_analysis = self._analyze_single_paper(paper, similarity_score)\n",
    "                if paper_analysis:\n",
    "                    analyzed_papers.append(paper_analysis)\n",
    "\n",
    "            ranked_papers = sorted(\n",
    "                analyzed_papers,\n",
    "                key=lambda x: x[\"scores\"][\"overall\"],\n",
    "                reverse=True\n",
    "            )\n",
    "\n",
    "            analysis_summary = self._generate_research_summary(ranked_papers)\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"query\": query,\n",
    "                \"summary\": analysis_summary,\n",
    "                \"papers\": ranked_papers\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Analysis failed: {str(e)}\", exc_info=True)\n",
    "            return {\"status\": \"error\", \"message\": f\"Analysis failed: {str(e)}\"}\n",
    "\n",
    "    def _analyze_single_paper(self, paper: ResearchPaper, similarity_score: float) -> Optional[Dict]:\n",
    "        \"\"\"Analyze a single research paper across multiple dimensions.\"\"\"\n",
    "        try:\n",
    "            current_year = datetime.now().year\n",
    "\n",
    "            citation_impact = self._calculate_citation_impact(\n",
    "                paper.citation_count,\n",
    "                paper.year or current_year\n",
    "            )\n",
    "\n",
    "            methodology_score = self._assess_methodology(paper.abstract or \"\")\n",
    "            recency_score = self._calculate_recency_score(paper.year or current_year)\n",
    "            venue_score = self._assess_venue_quality(paper.venue, paper.source)\n",
    "\n",
    "            overall_score = sum([\n",
    "                self.quality_weights[\"relevance\"] * similarity_score,\n",
    "                self.quality_weights[\"citation_impact\"] * citation_impact,\n",
    "                self.quality_weights[\"methodology\"] * methodology_score,\n",
    "                self.quality_weights[\"recency\"] * recency_score,\n",
    "                self.quality_weights[\"venue_quality\"] * venue_score\n",
    "            ])\n",
    "\n",
    "            return {\n",
    "                \"title\": paper.title,\n",
    "                \"year\": paper.year,\n",
    "                \"authors\": [author.name for author in paper.authors],\n",
    "                \"url\": str(paper.url) if paper.url else None,\n",
    "                \"source\": paper.source,\n",
    "                \"venue\": paper.venue,\n",
    "                \"scores\": {\n",
    "                    \"relevance\": similarity_score,\n",
    "                    \"citation_impact\": citation_impact,\n",
    "                    \"methodology\": methodology_score,\n",
    "                    \"recency\": recency_score,\n",
    "                    \"venue_quality\": venue_score,\n",
    "                    \"overall\": overall_score\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error analyzing paper: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _calculate_citation_impact(self, citations: int, year: int) -> float:\n",
    "        \"\"\"Calculate normalized citation impact score.\"\"\"\n",
    "        years_since_publication = max(1, datetime.now().year - year)\n",
    "        citations_per_year = citations / years_since_publication\n",
    "        return min(1.0, np.log1p(citations_per_year) / np.log1p(100))\n",
    "\n",
    "    def _assess_methodology(self, content: str) -> float:\n",
    "        \"\"\"Assess research methodology quality based on content analysis.\"\"\"\n",
    "        methodology_indicators = {\n",
    "            \"methodology\": 1.0,\n",
    "            \"experiment\": 0.8,\n",
    "            \"statistical analysis\": 0.8,\n",
    "            \"data collection\": 0.7,\n",
    "            \"sample size\": 0.7,\n",
    "            \"control group\": 0.9,\n",
    "            \"randomized\": 0.9,\n",
    "            \"validation\": 0.8\n",
    "        }\n",
    "        \n",
    "        content_lower = content.lower()\n",
    "        scored_indicators = [\n",
    "            weight for indicator, weight in methodology_indicators.items()\n",
    "            if indicator in content_lower\n",
    "        ]\n",
    "        \n",
    "        return sum(scored_indicators) / len(methodology_indicators) if scored_indicators else 0.5\n",
    "\n",
    "    def _calculate_recency_score(self, year: int) -> float:\n",
    "        \"\"\"Calculate recency score with exponential decay.\"\"\"\n",
    "        years_old = max(0, datetime.now().year - year)\n",
    "        return np.exp(-0.2 * years_old)\n",
    "\n",
    "    def _assess_venue_quality(self, venue: Optional[str], source: str) -> float:\n",
    "        \"\"\"Assess the quality of the publication venue.\"\"\"\n",
    "        venue_lower = venue.lower() if venue else \"\"\n",
    "        source_lower = source.lower() if source else \"\"\n",
    "\n",
    "        if any(journal in venue_lower for journal in [\"nature\", \"science\", \"cell\"]):\n",
    "            return 1.0\n",
    "        elif \"journal\" in venue_lower:\n",
    "            return 0.8\n",
    "        elif \"conference\" in venue_lower:\n",
    "            return 0.75\n",
    "        elif \"arxiv\" in source_lower:\n",
    "            return 0.7\n",
    "        else:\n",
    "            return 0.6\n",
    "\n",
    "    def _generate_research_summary(self, analyzed_papers: List[Dict]) -> Dict:\n",
    "        \"\"\"Generate a comprehensive summary of the research analysis.\"\"\"\n",
    "        if not analyzed_papers:\n",
    "            return {}\n",
    "\n",
    "        year_distribution = defaultdict(int)\n",
    "        source_distribution = defaultdict(int)\n",
    "        avg_scores = defaultdict(float)\n",
    "\n",
    "        for paper in analyzed_papers:\n",
    "            if paper.get('year'):\n",
    "                year_distribution[paper['year']] += 1\n",
    "            source_distribution[paper['source']] += 1\n",
    "            \n",
    "            for score_type, score in paper['scores'].items():\n",
    "                avg_scores[score_type] += score\n",
    "\n",
    "        total_papers = len(analyzed_papers)\n",
    "        for score_type in avg_scores:\n",
    "            avg_scores[score_type] /= total_papers\n",
    "\n",
    "        return {\n",
    "            \"total_papers\": total_papers,\n",
    "            \"year_range\": {\n",
    "                \"oldest\": min(year_distribution.keys()) if year_distribution else None,\n",
    "                \"newest\": max(year_distribution.keys()) if year_distribution else None\n",
    "            },\n",
    "            \"source_distribution\": dict(source_distribution),\n",
    "            \"average_scores\": dict(avg_scores),\n",
    "            \"top_venues\": [\n",
    "                paper['venue'] for paper in analyzed_papers[:3]\n",
    "                if paper.get('venue')\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Cell 5: Research Interface and Analysis Display\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "\n",
    "class ResearchInterface:\n",
    "    \"\"\"Interface for collecting and analyzing research papers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.paper_collector = PaperCollector()\n",
    "        self.analyzer = ResearchAnalyzer(self.paper_collector)\n",
    "        self.logger = logging.getLogger(__name__ + \".ResearchInterface\")\n",
    "\n",
    "    async def search_and_analyze(self):\n",
    "        \"\"\"Interactive research paper search and analysis.\"\"\"\n",
    "        print(\"\\nAdvanced Research Paper Analysis\")\n",
    "        print(\"===============================\")\n",
    "        \n",
    "        query = input(\"\\nEnter your research query (use 'and' to combine specific concepts): \")\n",
    "        max_results = input(\"Enter maximum number of papers to analyze (default 30): \")\n",
    "        max_results = int(max_results) if max_results.isdigit() else 30\n",
    "\n",
    "        print(\"\\nCollecting and analyzing papers...\")\n",
    "        analysis_results = await self.analyzer.analyze_research_topic(query, max_results)\n",
    "\n",
    "        if analysis_results[\"status\"] == \"success\":\n",
    "            self._display_analysis_results(analysis_results)\n",
    "            return analysis_results\n",
    "        else:\n",
    "            print(f\"\\nError: {analysis_results['message']}\")\n",
    "            return None\n",
    "\n",
    "    def _display_analysis_results(self, results):\n",
    "        \"\"\"Display analyzed papers in a structured format.\"\"\"\n",
    "        summary = results[\"summary\"]\n",
    "        papers = results[\"papers\"]\n",
    "\n",
    "        # Display summary statistics\n",
    "        print(\"\\nAnalysis Summary\")\n",
    "        print(\"--------------\")\n",
    "        print(f\"Total Papers Analyzed: {summary['total_papers']}\")\n",
    "        if summary['year_range']['oldest'] and summary['year_range']['newest']:\n",
    "            print(f\"Year Range: {summary['year_range']['oldest']} - {summary['year_range']['newest']}\")\n",
    "        print(\"\\nSource Distribution:\")\n",
    "        for source, count in summary['source_distribution'].items():\n",
    "            print(f\"  {source}: {count} papers\")\n",
    "\n",
    "        # Create DataFrame for detailed paper analysis\n",
    "        paper_data = []\n",
    "        for paper in papers:\n",
    "            paper_data.append({\n",
    "                \"Title\": paper[\"title\"],\n",
    "                \"Year\": paper[\"year\"],\n",
    "                \"Authors\": \"; \".join(paper[\"authors\"]),\n",
    "                \"Venue\": paper[\"venue\"] or \"N/A\",\n",
    "                \"Overall Score\": f\"{paper['scores']['overall']:.3f}\",\n",
    "                \"Relevance\": f\"{paper['scores']['relevance']:.3f}\",\n",
    "                \"Citation Impact\": f\"{paper['scores']['citation_impact']:.3f}\",\n",
    "                \"Methodology\": f\"{paper['scores']['methodology']:.3f}\",\n",
    "                \"URL\": paper[\"url\"] or \"N/A\"\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(paper_data)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\nRanked Papers (sorted by overall score)\")\n",
    "        print(\"-------------------------------------\")\n",
    "        \n",
    "        # Configure pandas display options\n",
    "        pd.set_option('display.max_colwidth', None)\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        \n",
    "        # Create styled DataFrame\n",
    "        styled_df = df.style.background_gradient(subset=['Overall Score'], cmap='YlOrRd')\n",
    "        display(HTML(styled_df.to_html(escape=False)))\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 7\n",
    "- User interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 15:44:18,915 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advanced Research Paper Analysis\n",
      "===============================\n",
      "\n",
      "Collecting and analyzing papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 15:44:37,804 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:44:38,998 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=30&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:44:40,512 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=60&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:44:41,998 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=90&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:44:43,343 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=120&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:44:45,413 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=150&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:44:47,330 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=180&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:44:48,567 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=210&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:44:49,906 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=240&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:44:51,500 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=270&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:44:53,572 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=300&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:44:55,259 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=330&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:44:56,418 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=360&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:44:58,384 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=390&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:44:59,851 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=420&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:00,930 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=450&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:02,173 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=480&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:04,018 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=510&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:05,176 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=540&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:06,310 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=570&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:09,247 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=600&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:10,281 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=630&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:11,481 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=660&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:12,721 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=690&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:14,154 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=720&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:15,690 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=750&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:17,007 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=780&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:24,678 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=810&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:26,136 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=840&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:27,315 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=870&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:28,490 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=900&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:29,789 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=930&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:31,052 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=960&limit=30 \"HTTP/1.1 200 OK\"\n",
      "2025-02-12 15:45:31,353 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Mitophagy%20and%20mitochondrial%20quality%20control&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=990&limit=30 \"HTTP/1.1 400 Bad Request\"\n",
      "Error fetching from Semantic Scholar: Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\n",
      "2025-02-12 15:45:31,357 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=Mitophagy+and+mitochondrial+quality+control&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "2025-02-12 15:45:32,497 - INFO - Got first page: 100 of 2595038 total results\n",
      "2025-02-12 15:45:33,406 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Error storing in vector database: 'Chroma' object has no attribute 'persist'\n",
      "Analysis failed: 'Chroma' object has no attribute 'similarity_search_with_scores'\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/j7/smpqy2fn30l7j5kcqh76jqp40000gn/T/ipykernel_9334/18856491.py\", line 34, in analyze_research_topic\n",
      "    vector_results = self.paper_collector.vector_store.similarity_search_with_scores(\n",
      "AttributeError: 'Chroma' object has no attribute 'similarity_search_with_scores'. Did you mean: 'similarity_search_with_score'?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error: Analysis failed: 'Chroma' object has no attribute 'similarity_search_with_scores'\n"
     ]
    }
   ],
   "source": [
    "#%% Cell 7: Run the interface\n",
    "interface = ResearchInterface()\n",
    "results = await interface.search_and_analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (article_env)",
   "language": "python",
   "name": "article_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
